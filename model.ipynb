{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim \n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F \n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class n_convs(nn.Module):\n",
    "    def __init__(self, n_layer=2, in_channels=1, out_channels=64, imsize=64, kernelsize=2, stride=1, padding=0):\n",
    "        \n",
    "        super(n_convs, self).__init__()\n",
    "        \n",
    "        self.n_layer = n_layer \n",
    "        self.layer_list = []\n",
    "        self.layer_list.append(nn.Conv2d(in_channels, out_channels, kernelsize, stride, padding))\n",
    "        \n",
    "        for i in range(n_layer-1):\n",
    "            self.layer_list.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            self.layer_list.append(nn.Conv2d(out_channels, out_channels, kernelsize, stride, padding))\n",
    "        self.layer_list.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        for i in range(self.n_layer):\n",
    "            x = self.layer_list[i](x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Conv2d(1, 64, kernel_size=(2, 2), stride=(1, 1)), LeakyReLU(0.2, inplace), Conv2d(64, 64, kernel_size=(2, 2), stride=(1, 1)), LeakyReLU(0.2, inplace)]\n"
     ]
    }
   ],
   "source": [
    "double_conv = n_convs()\n",
    "a = torch.rand(1, 1, 64, 64)\n",
    "x = torch.autograd.Variable(a)\n",
    "# double_conv(x)\n",
    "print(double_conv.layer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ConvSet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernersize, stride, padding, bias=False, \n",
    "                 batchNorm=True, activation='ReLU', name='Default_net'):\n",
    "        \n",
    "        super(ConvSet, self).__init__()\n",
    "        \n",
    "        assert activation in ['ReLU', 'LeakyReLU'], \"Activation methods not implemented!!!\"\n",
    "        self.convSet = nn.Sequential()\n",
    "        self.nameList = []\n",
    "        \n",
    "        conv_name = name + '.{}-{}.conv'.format(in_channels, out_channels)\n",
    "        activ_name = name + '.{}.' + activation\n",
    "        activ_name = activ_name.format(out_channels)\n",
    "        if batchNorm:\n",
    "            batch_name = name + '.{}.batchnorm'.format(out_channels)\n",
    "            \n",
    "        self.convSet.add_module(conv_name,\n",
    "                               nn.Conv2d(in_channels, out_channels, kernersize, stride, padding, bias=bias))\n",
    "        self.nameList.append(conv_name)\n",
    "        \n",
    "        if batchNorm:\n",
    "            self.convSet.add_module(batch_name,\n",
    "                                   nn.BatchNorm2d(out_channels))\n",
    "            self.nameList.append(batch_name)\n",
    "        \n",
    "        if activation == 'ReLU':\n",
    "            self.convSet.add_module(activ_name,\n",
    "                                   nn.ReLU(inplace=True))\n",
    "        elif activation == 'LeakyReLU':\n",
    "            self.convSet.add_module(activ_name,\n",
    "                                   nn.LeakyReLU(0.2, inplace=True))\n",
    "        self.nameList.append(activ_name)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convSet(x)\n",
    "        return x\n",
    "    \n",
    "    def getEntriesAndNames(self):\n",
    "        moduleList = list(self.convSet)\n",
    "        return moduleList, self.nameList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "conv = ConvSet(1, 64, 4, 2, 1)\n",
    "a = torch.rand(2, 1, 64, 64)\n",
    "x = torch.autograd.Variable(a)\n",
    "y = conv(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class GeomEncoder(nn.Module):\n",
    "    def __init__(self, encode_dim=20, in_channels=1, imsize=64, nfeat=64,  extra_layer=0, batchNorm=True, activation='ReLU'):\n",
    "        \n",
    "        super(GeomEncoder, self).__init__()  \n",
    "        # initial input\n",
    "        self.initial = ConvSet(in_channels, nfeat, 4, 2, 1, False, \n",
    "                               batchNorm=batchNorm, activation=activation, name='initial')\n",
    "        \n",
    "        # pyramid structure\n",
    "        encoder_list = []\n",
    "        name_list = []\n",
    "        c_imsize, c_feat = imsize / 2, nfeat\n",
    "        \n",
    "        ind = 0       \n",
    "        while c_imsize >= 4:\n",
    "            in_feat = c_feat\n",
    "            out_feat = c_feat * 2\n",
    "            \n",
    "            ind += 1\n",
    "            layer_name = 'pyramid_' + str(ind)\n",
    "            convnet = ConvSet(in_feat, out_feat, 4, 2, 1, bias=False, \n",
    "                                            batchNorm=batchNorm, activation=activation, name=layer_name)\n",
    "            \n",
    "            entries, names = convnet.getEntriesAndNames()\n",
    "            encoder_list.extend(entries)\n",
    "            name_list.extend(names)\n",
    "        \n",
    "            c_feat *= 2\n",
    "            c_imsize = c_imsize / 4\n",
    "        # Tensor[None, 256, 8, 8]\n",
    " \n",
    "        # final convolutional layer, out_feat=20 is to be changed\n",
    "        final_conv = ConvSet(c_feat, 20, 4, 2, 1, bias=False, \n",
    "                                            batchNorm=batchNorm, activation=activation, name='final_conv')\n",
    "        entries, names = final_conv.getEntriesAndNames()\n",
    "        encoder_list.extend(entries)\n",
    "        name_list.extend(names)\n",
    "        \n",
    "        encoder_dict = OrderedDict(zip(name_list, encoder_list))\n",
    "        self.encoder =  nn.Sequential(encoder_dict)\n",
    "        # Tensor[None, 20, 4, 4] -> 320\n",
    "        \n",
    "        self.final_layer = nn.Sequential()\n",
    "        self.final_layer.add_module('encoded',\n",
    "                                    nn.Linear(320, encode_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        x = self.encoder(x)\n",
    "        x = self.final_layer(x.view(2, 320))\n",
    "        return x\n",
    "    \n",
    "    def get_num_params(self):\n",
    "        model_parameters = filter(lambda p: p.requires_grad, self.parameters())\n",
    "        params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeomEncoder(\n",
      "  (initial): ConvSet(\n",
      "    (convSet): Sequential(\n",
      "      (initial.1-64.conv): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (initial.64.LeakyReLU): LeakyReLU(0.2, inplace)\n",
      "    )\n",
      "  )\n",
      "  (encoder): Sequential(\n",
      "    (pyramid_1.64-128.conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid_1.128.LeakyReLU): LeakyReLU(0.2, inplace)\n",
      "    (pyramid_2.128-256.conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid_2.256.LeakyReLU): LeakyReLU(0.2, inplace)\n",
      "    (final_conv.256-20.conv): Conv2d(256, 20, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (final_conv.20.LeakyReLU): LeakyReLU(0.2, inplace)\n",
      "  )\n",
      "  (final_layer): Sequential(\n",
      "    (encoded): Linear(in_features=320, out_features=50, bias=True)\n",
      "  )\n",
      ")\n",
      "torch.Size([2, 50])\n",
      "754354\n"
     ]
    }
   ],
   "source": [
    "encoder = GeomEncoder(encode_dim=50, activation='LeakyReLU', batchNorm=False)\n",
    "        \n",
    "print(encoder)\n",
    "a = torch.rand(2, 1, 64, 64)\n",
    "x = torch.autograd.Variable(a)\n",
    "y = encoder(x)\n",
    "print(y.shape)\n",
    "# for p in encoder.parameters():\n",
    "#     print(p)in_channels\n",
    "print(encoder.get_num_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class TConvSet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernersize, stride, padding, bias=False, \n",
    "                 batchNorm=True, activation='ReLU', name='Default_net'):\n",
    "        \n",
    "        super(TConvSet, self).__init__()\n",
    "        \n",
    "        assert activation in ['ReLU', 'LeakyReLU'], \"Activation methods not implemented!!!\"\n",
    "        self.convSet = nn.Sequential()\n",
    "        self.nameList = []\n",
    "        \n",
    "        conv_name = name + '.{}-{}.transconv'.format(in_channels, out_channels)\n",
    "        activ_name = name + '.{}.' + activation\n",
    "        activ_name = activ_name.format(out_channels)\n",
    "        if batchNorm:\n",
    "            batch_name = name + '.{}.batchnorm'.format(out_channels)\n",
    "            \n",
    "        self.convSet.add_module(conv_name,\n",
    "                               nn.ConvTranspose2d(in_channels, out_channels, kernersize, stride, padding, bias=bias))\n",
    "        self.nameList.append(conv_name)\n",
    "        \n",
    "        if batchNorm:\n",
    "            self.convSet.add_module(batch_name,\n",
    "                                   nn.BatchNorm2d(out_channels))\n",
    "            self.nameList.append(batch_name)\n",
    "        \n",
    "        if activation == 'ReLU':\n",
    "            self.convSet.add_module(activ_name,\n",
    "                                   nn.ReLU(inplace=True))\n",
    "        elif activation == 'LeakyReLU':\n",
    "            self.convSet.add_module(activ_name,\n",
    "                                   nn.LeakyReLU(0.2, inplace=True))\n",
    "        self.nameList.append(activ_name)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convSet(x)\n",
    "        return x\n",
    "    \n",
    "    def getEntriesAndNames(self):\n",
    "        moduleList = list(self.convSet)\n",
    "        return moduleList, self.nameList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 64, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "tconv = TConvSet(1, 64, 4, 2, 1)\n",
    "a = torch.rand(2, 1, 64, 64)\n",
    "x = torch.autograd.Variable(a)\n",
    "y = tconv(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "tconv = TConvSet(20, 128, 4, 1, 0)\n",
    "a = torch.rand(2, 20, 1, 1)\n",
    "x = torch.autograd.Variable(a)\n",
    "y = tconv(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GeomDecoder(nn.Module):\n",
    "    def __init__(self, encode_dim=20, noise_dim=20, out_channels=1, imsize=64, nfeat=64, extra_layer=0, batchNorm=True, activation='ReLU'):\n",
    "        \n",
    "        super(GeomDecoder, self).__init__() \n",
    "        \n",
    "        self.noise_dim = noise_dim\n",
    "        self.encode_dim = encode_dim\n",
    "        \n",
    "        cngf, tisize = nfeat//2, 4\n",
    "        while tisize != imsize:\n",
    "            cngf = cngf * 2\n",
    "            tisize = tisize * 2\n",
    "            \n",
    "        nz = noise_dim + encode_dim\n",
    "        \n",
    "        # initial input\n",
    "        self.initial = TConvSet(nz, cngf, 4, 1, 0, False, \n",
    "                               batchNorm=batchNorm, activation=activation, name='initial')\n",
    "        \n",
    "        # pyramid structure\n",
    "        decoder_list = []\n",
    "        name_list = []        \n",
    "        c_imsize, c_feat = 4, cngf\n",
    "        \n",
    "        ind = 0\n",
    "        while c_imsize < imsize // 2:\n",
    "            ind += 1\n",
    "            layer_name = 'pyramid_' + str(ind)\n",
    "            tconvnet = TConvSet(c_feat, c_feat//2, 4, 2, 1, bias=False,\n",
    "                               batchNorm=batchNorm, activation=activation, name=layer_name)\n",
    "            \n",
    "            entries, names = tconvnet.getEntriesAndNames()\n",
    "            decoder_list.extend(entries)\n",
    "            name_list.extend(names)\n",
    "            \n",
    "            c_feat = c_feat // 2\n",
    "            c_imsize = c_imsize * 2\n",
    "        \n",
    "        # extra layer\n",
    "        ind = 0\n",
    "        for i in range(extra_layer):\n",
    "            ind += 1\n",
    "            layer_name = 'extra_layer_' + str(ind)\n",
    "            extra_tconvnet = ConvSet(c_feat, c_feat, 3, 1, 1, bias=False, name=layer_name)\n",
    "            \n",
    "            entries, names = extra_tconvnet.getEntriesAndNames()\n",
    "            decoder_list.extend(entries)\n",
    "            name_list.extend(names)       \n",
    "        \n",
    "        # final output layer\n",
    "        final_tconvnet = TConvSet(c_feat, out_channels, 4, 2, 1, bias=False,\n",
    "                   batchNorm=batchNorm, activation=activation, name='final_layer')\n",
    "\n",
    "        entries, names = final_tconvnet.getEntriesAndNames()\n",
    "        decoder_list.extend(entries)\n",
    "        name_list.extend(names)\n",
    "        \n",
    "        decoder_dict = OrderedDict(zip(name_list, decoder_list))\n",
    "        self.decoder =  nn.Sequential(decoder_dict)   \n",
    "        \n",
    "        \n",
    "    def forward(self, *args):\n",
    "        if len(args) == 2:\n",
    "            x, noise = args[0], args[1]\n",
    "            assert noise.shape[1] == self.noise_dim and x.shape[1] == self.encode_dim, \"Dimension of noise or encoded vector does not match\"\n",
    "            x = torch.cat((x, noise), dim=1)\n",
    "        else:\n",
    "            x = args[0]\n",
    "            assert self.noise_dim == 0, \"Model needs noise with dimension dim={} as input\".format(self.noise_dim)\n",
    "            assert x.shape[1] == self.encode_dim, \"Dimension of encoded vector does not match\"\n",
    "        \n",
    "        x = x.view((x.shape[0], x.shape[1], 1, 1))\n",
    "        x = self.initial(x)        \n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def get_num_params(self):\n",
    "        model_parameters = filter(lambda p: p.requires_grad, self.parameters())\n",
    "        params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "        return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeomDecoder(\n",
      "  (initial): TConvSet(\n",
      "    (convSet): Sequential(\n",
      "      (initial.20-512.transconv): ConvTranspose2d(20, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "      (initial.512.batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (initial.512.ReLU): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (pyramid_1.512-256.transconv): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid_1.256.batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (pyramid_1.256.ReLU): ReLU(inplace)\n",
      "    (pyramid_2.256-128.transconv): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid_2.128.batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (pyramid_2.128.ReLU): ReLU(inplace)\n",
      "    (pyramid_3.128-64.transconv): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (pyramid_3.64.batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (pyramid_3.64.ReLU): ReLU(inplace)\n",
      "    (extra_layer_1.64-64.conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (extra_layer_1.64.batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (extra_layer_1.64.ReLU): ReLU(inplace)\n",
      "    (extra_layer_2.64-64.conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (extra_layer_2.64.batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (extra_layer_2.64.ReLU): ReLU(inplace)\n",
      "    (extra_layer_3.64-64.conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (extra_layer_3.64.batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (extra_layer_3.64.ReLU): ReLU(inplace)\n",
      "    (extra_layer_4.64-64.conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (extra_layer_4.64.batchnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (extra_layer_4.64.ReLU): ReLU(inplace)\n",
      "    (final_layer.64-1.transconv): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (final_layer.1.batchnorm): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (final_layer.1.ReLU): ReLU(inplace)\n",
      "  )\n",
      ")\n",
      "torch.Size([6, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "decoder = GeomDecoder(20, 0, 1, 64, 64, 4, True)\n",
    "print(decoder)\n",
    "a = torch.ones(6, 20)\n",
    "noise = torch.rand(6, 10)\n",
    "x = torch.autograd.Variable(a)\n",
    "noise = torch.autograd.Variable(noise)\n",
    "y = decoder(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
